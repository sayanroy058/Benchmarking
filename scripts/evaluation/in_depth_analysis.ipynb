{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import geopandas as gpd\n",
    "\n",
    "import torch\n",
    "from torch_geometric.profile import count_parameters\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "# Add the 'scripts' directory to Python Path\n",
    "scripts_path=os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "import evaluation.help_functions as hf\n",
    "import evaluation.plot_functions as pf\n",
    "\n",
    "import gnn.gnn_io as gio\n",
    "from gnn.help_functions import compute_spearman_pearson, compute_r2_torch\n",
    "from gnn.models.trans_conv import TransConv\n",
    "from training.help_functions import seed_worker, normalize_x_features_with_scaler, normalize_dataset\n",
    "from data_preprocessing.help_functions import highway_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute path to the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "# Paths, adjust as needed\n",
    "run_path = os.path.join(project_root, \"data\", \"TR-C_Benchmarks\", \"tc_54x_part_4\")\n",
    "districts = gpd.read_file(os.path.join(project_root, \"data\", \"visualisation\", \"districts_paris.geojson\"))\n",
    "base_case_path = os.path.join(project_root, \"data\", \"links_and_stats\", \"pop_1pct_basecase_average_output_links.geojson\")\n",
    "result_path = 'results/'\n",
    "\n",
    "# GNN Parameters (Others are default values)\n",
    "in_channels = 5\n",
    "out_channels = 1\n",
    "use_dropout = False\n",
    "use_graph_norm = True\n",
    "use_residuals = True\n",
    "num_heads = 4\n",
    "hidden_channels = [32,64,128,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,256,128,64,32]\n",
    "\n",
    "links_base_case = gpd.read_file(base_case_path, crs=\"EPSG:4326\")\n",
    "data_created_during_training = os.path.join(run_path, 'data_created_during_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Load test data from the run itself! ###\n",
    "###########################################\n",
    "\n",
    "# Load scalers\n",
    "scaler_x = joblib.load(os.path.join(data_created_during_training, 'test_x_scaler.pkl'))\n",
    "scaler_pos = joblib.load(os.path.join(data_created_during_training, 'test_pos_scaler.pkl'))\n",
    "\n",
    "# Load the test dataset created during training\n",
    "test_set_dl = torch.load(os.path.join(data_created_during_training, 'test_dl.pt'))\n",
    "\n",
    "# Load the DataLoader parameters\n",
    "with open(os.path.join(data_created_during_training, 'test_loader_params.json'), 'r') as f:\n",
    "    test_set_dl_loader_params = json.load(f)\n",
    "    \n",
    "# Remove or correct collate_fn if it is incorrectly specified\n",
    "if 'collate_fn' in test_set_dl_loader_params and isinstance(test_set_dl_loader_params['collate_fn'], str):\n",
    "    del test_set_dl_loader_params['collate_fn']  # Remove it to use the default collate function\n",
    "    \n",
    "test_set_loader = torch.utils.data.DataLoader(test_set_dl, **test_set_dl_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransConv(use_dropout=use_dropout, use_graph_norm=use_graph_norm, use_residuals=use_residuals,\n",
    "                  in_channels=in_channels, out_channels=out_channels, num_heads=num_heads,\n",
    "                  hidden_channels=hidden_channels)\n",
    "\n",
    "print(f\"Trainable model parameters: {round(count_parameters(model) / 1e6, 2)} M\")\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_path = os.path.join(run_path, 'trained_model/model.pth')\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fct = torch.nn.MSELoss().to(dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gdfs for the entire test set\n",
    "gdfs = []\n",
    "\n",
    "for i in tqdm(range(len(test_set_loader.dataset))):\n",
    "    my_test_data = test_set_loader.dataset[i]\n",
    "    my_test_x = test_set_loader.dataset[i].x\n",
    "    my_test_x = my_test_x.to('cpu')\n",
    "    \n",
    "    test_loss_my_test_data, r_squared_my_test_data, actual_vals_my_test_data, predictions_my_test_data, baseline_loss_my_test_data = hf.validate_model_on_test_set(model, my_test_data, loss_fct, device)\n",
    "    inversed_x = scaler_x.inverse_transform(my_test_x)\n",
    "    \n",
    "    gdf = hf.data_to_geodataframe_with_og_values(data=my_test_data, original_gdf=links_base_case, predicted_values=predictions_my_test_data, inversed_x=inversed_x)\n",
    "    \n",
    "    # gdf = gpd.sjoin(gdf, districts, how='left', op='intersects')\n",
    "    # gdf = gdf.rename(columns={\"c_ar\": \"district\"})\n",
    "    \n",
    "    gdf['capacity_reduction_rounded'] = gdf['capacity_reduction'].round(decimals=3)\n",
    "    gdf['highway'] = gdf['highway'].map(highway_mapping)\n",
    "\n",
    "    ## In case the duplicate indices still exist\n",
    "    # gdf = gdf[~gdf.index.duplicated(keep='first')]\n",
    "    \n",
    "    gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Times Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average inference time\n",
    "# Across the test set\n",
    "\n",
    "times = []\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "\n",
    "    for data in test_set_loader.dataset:\n",
    "        \n",
    "        data = data.to(device)\n",
    "    \n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        starter.record()\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()  # Wait for all kernels to finish\n",
    "        inference_time = starter.elapsed_time(ender)  # In milliseconds\n",
    "        times.append(inference_time)\n",
    "\n",
    "average_t = sum(times) / len(times)\n",
    "print(f\"Average inference time (across test set): {average_t:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average inference time for Batch Processing\n",
    "batch_times = []\n",
    "\n",
    "# Create a new DataLoader with custom batch size\n",
    "batch_size = 32\n",
    "batched_test_loader = torch.utils.data.DataLoader(test_set_dl, batch_size, collate_fn=gio.collate_fn, shuffle=True,\n",
    "                                                  num_workers = 4, worker_init_fn=seed_worker)\n",
    "\n",
    "model.eval()\n",
    "with torch.inference_mode():\n",
    "    \n",
    "    for batch in batched_test_loader:\n",
    "        \n",
    "        batch = batch.to(device)\n",
    "\n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        starter.record()\n",
    "\n",
    "        output = model(batch)\n",
    "\n",
    "        ender.record()\n",
    "        torch.cuda.synchronize()\n",
    "        batch_time = starter.elapsed_time(ender)  # In milliseconds\n",
    "        batch_times.append(batch_time)\n",
    "\n",
    "average_batch_t = sum(batch_times) / len(batch_times)\n",
    "print(f\"Average batch inference time: {average_batch_t:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis by Road Type and Capacity Reduction (Relevant for Paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confidence_intervals(base_variances, actual_vals, predictions, n_bootstraps=1024, plot_results=False):\n",
    "\n",
    "    # 1. Init (base variances, actual_vals and predictions)\n",
    "    actual_vals = torch.tensor(actual_vals, dtype=torch.float32).to(device)\n",
    "    predictions = torch.tensor(predictions, dtype=torch.float32).to(device)\n",
    "\n",
    "    mse_loss = torch.nn.MSELoss(reduction='none').to(dtype=torch.float32).to(device)\n",
    "    mse_values = mse_loss(predictions, actual_vals).cpu().numpy().flatten()\n",
    "    # mse_values = torch.split(mse_values, 31635, dim=0)\n",
    "    # mse_values = torch.mean(torch.stack(mse_values, dim=0), dim=0).cpu().numpy().flatten()\n",
    "\n",
    "    # 2. Ensure arrays are of equal length, and filter by road type\n",
    "    assert base_variances.shape[0] == mse_values.shape[0], \"Arrays must be of same length\"\n",
    "\n",
    "    n = base_variances.shape[0]\n",
    "\n",
    "    # 3. Number of bootstrap iterations\n",
    "    ## Defined in function parameters\n",
    "\n",
    "    # 4. Create lists to store the bootstrap means and ratios\n",
    "    boot_sigma2 = []\n",
    "    boot_mse = []\n",
    "    boot_share = []\n",
    "\n",
    "    for _ in range(n_bootstraps):\n",
    "        # 5. Sample indices with replacement from the full dataset\n",
    "        indices = np.random.choice(n, size=n, replace=True)\n",
    "\n",
    "        # 6. Create bootstrap sample using these indices\n",
    "        sample_sigma2 = base_variances[indices]\n",
    "        sample_mse = mse_values[indices]\n",
    "\n",
    "        # 7. Compute mean values of sigma² and MSE in the sample\n",
    "        mean_sigma2 = np.mean(sample_sigma2)\n",
    "        mean_mse = np.mean(sample_mse)\n",
    "\n",
    "        # 8. Compute the ratio (only if denominator is non-zero to avoid div-by-zero)\n",
    "        if mean_mse > 0:\n",
    "            share = 100*(mean_sigma2 / mean_mse)\n",
    "            boot_sigma2.append(mean_sigma2)\n",
    "            boot_mse.append(mean_mse)\n",
    "            boot_share.append(share)\n",
    "        else:\n",
    "            # Optional: print or log warning if a pathological case occurs\n",
    "            continue\n",
    "\n",
    "    # 9. Convert lists to arrays (optional but convenient)\n",
    "    boot_sigma2 = np.array(boot_sigma2)\n",
    "    boot_mse = np.array(boot_mse)\n",
    "    boot_share = np.array(boot_share)\n",
    "\n",
    "    # 10. Compute 95% confidence intervals (percentile-based bootstrap CI)\n",
    "    ci_sigma2 = np.percentile(boot_sigma2, [2.5, 97.5])\n",
    "    ci_mse = np.percentile(boot_mse, [2.5, 97.5])\n",
    "    ci_share = np.percentile(boot_share, [2.5, 97.5])\n",
    "\n",
    "    # # 11. Print final results\n",
    "    # print(f\"σ² CI:  [{ci_sigma2[0]:.2f}, {ci_sigma2[1]:.2f}]\")\n",
    "    # print(f\"MSE CI: [{ci_mse[0]:.2f}, {ci_mse[1]:.2f}]\")\n",
    "    # print(f\"Share CI: [{ci_share[0]:.2f}, {ci_share[1]:.2f}]\")\n",
    "\n",
    "    if plot_results:\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "        # Histogram of base-case variance\n",
    "        axs[0].hist(boot_sigma2, bins=30, color='skyblue', edgecolor='black')\n",
    "        axs[0].axvline(ci_sigma2[0], color='blue', linestyle='--', label='95% CI')\n",
    "        axs[0].axvline(ci_sigma2[1], color='blue', linestyle='--')\n",
    "        axs[0].set_title(\"Bootstrapped Mean Base Case Variance ($\\sigma^2_{t,b}$)\")\n",
    "        axs[0].set_xlabel(\"Mean Variance\")\n",
    "        axs[0].set_ylabel(\"Frequency\")\n",
    "        axs[0].legend()\n",
    "\n",
    "        # Histogram of MSE\n",
    "        axs[1].hist(boot_mse, bins=30, color='salmon', edgecolor='black')\n",
    "        axs[1].axvline(ci_mse[0], color='red', linestyle='--', label='95% CI')\n",
    "        axs[1].axvline(ci_mse[1], color='red', linestyle='--')\n",
    "        axs[1].set_title(\"Bootstrapped Mean MSE\")\n",
    "        axs[1].set_xlabel(\"Mean MSE\")\n",
    "        axs[1].set_ylabel(\"Frequency\")\n",
    "        axs[1].legend()\n",
    "\n",
    "        # Histogram of share\n",
    "        axs[2].hist(boot_share, bins=30, color='seagreen', edgecolor='black')\n",
    "        axs[2].axvline(ci_share[0], color='darkgreen', linestyle='--', label='95% CI')\n",
    "        axs[2].axvline(ci_share[1], color='darkgreen', linestyle='--')\n",
    "        axs[2].set_title(\"Bootstrapped Share ($\\sigma^2_{t,b} / \\mathrm{MSE}$)\")\n",
    "        axs[2].set_xlabel(\"Share\")\n",
    "        axs[2].set_ylabel(\"Frequency\")\n",
    "        axs[2].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return {'ci_sigma2': ci_sigma2, 'ci_mse': ci_mse, 'ci_share': ci_share}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model_with_interpretable_error(road_type, gdfs):\n",
    "    \"\"\"\n",
    "    Validate model performance across all test set observations for specific road types.\n",
    "    \n",
    "    Args:\n",
    "        road_type (str)\n",
    "        gdfs: List of GeoDataFrames, each representing one test set observation\n",
    "    \"\"\"\n",
    "    loss_fct_l1 = torch.nn.L1Loss()\n",
    "    loss_fct_l2 = torch.nn.MSELoss()\n",
    "    \n",
    "    # Initialize lists to store values across all observations\n",
    "    all_actual_vals = []\n",
    "    all_predicted_vals = []\n",
    "    mean_car_vols = []\n",
    "    all_car_vols = []\n",
    "    \n",
    "    variances_base_case = []\n",
    "    all_base_case_var = []\n",
    "    \n",
    "    variances = []\n",
    "    std_devs = []\n",
    "    std_dev_multiplied = []\n",
    "    cv_percents = []\n",
    "    \n",
    "    # Collect values from all GDFs\n",
    "    i = 0\n",
    "    for gdf in gdfs:\n",
    "        indices = hf.get_road_type_indices(gdf)[road_type]\n",
    "        \n",
    "        if len(indices) > 0:  # Only process if we have roads of this type\n",
    "            i += 1\n",
    "            actual_vals = gdf.loc[indices, 'vol_car_change_actual']\n",
    "            predicted_vals = gdf.loc[indices, 'vol_car_change_predicted']\n",
    "            \n",
    "            all_actual_vals.extend(actual_vals.to_numpy())\n",
    "            all_predicted_vals.extend(predicted_vals.to_numpy())\n",
    "            all_base_case_var.extend(gdf.loc[indices, 'variance_base_case'])\n",
    "            all_car_vols.extend(gdf.loc[indices, 'vol_base_case'])\n",
    "            \n",
    "            # Collect statistics\n",
    "            mean_car_vols.append(gdf.loc[indices, 'vol_base_case'].mean())\n",
    "            \n",
    "            car_volumes = actual_vals.to_numpy()\n",
    "            car_volume_variance = np.var(car_volumes)\n",
    "            \n",
    "            variances.append(car_volume_variance)\n",
    "            variances_base_case.append(gdf.loc[indices, 'variance_base_case'].mean())\n",
    "            \n",
    "            std_devs.append(gdf.loc[indices, 'std_dev'].mean())\n",
    "            std_dev_multiplied.append(gdf.loc[indices, 'std_dev_multiplied'].mean())\n",
    "            cv_percents.append(gdf.loc[indices, 'cv_percent'].mean())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_actual_vals = np.array(all_actual_vals)\n",
    "    all_predicted_vals = np.array(all_predicted_vals)\n",
    "    all_base_case_var = np.array(all_base_case_var)\n",
    "    all_car_vols = np.array(all_car_vols)\n",
    "    \n",
    "    actual_mean = torch.mean(torch.tensor(all_actual_vals))\n",
    "    \n",
    "    # Calculate metrics\n",
    "    spearman_corr, pearson_corr = compute_spearman_pearson(all_predicted_vals, all_actual_vals, is_np=True)    \n",
    "    r_squared = compute_r2_torch(preds=torch.tensor(all_predicted_vals), targets=torch.tensor(all_actual_vals))\n",
    "    \n",
    "    l1_loss = loss_fct_l1(torch.tensor(all_actual_vals), torch.tensor(all_predicted_vals))\n",
    "    l2_loss = loss_fct_l2(torch.tensor(all_actual_vals), torch.tensor(all_predicted_vals))\n",
    "    \n",
    "    l1_naive = loss_fct_l1(torch.tensor(all_actual_vals), torch.full_like(torch.tensor(all_actual_vals), actual_mean))    \n",
    "    l2_naive = loss_fct_l2(torch.tensor(all_actual_vals), torch.full_like(torch.tensor(all_actual_vals), actual_mean))\n",
    "    \n",
    "    # Calculate averages of statistics\n",
    "    mean_car_vol = np.mean(mean_car_vols)\n",
    "    variance = np.mean(variances)\n",
    "    variance_base_case = np.mean(variances_base_case)\n",
    "    std_dev = np.mean(std_devs)\n",
    "    std_dev_multiplied = np.mean(std_dev_multiplied)\n",
    "    cv_percent = np.mean(cv_percents)\n",
    "\n",
    "    # Get confidence intervals\n",
    "    # ci_results = get_confidence_intervals(all_base_case_var,all_actual_vals,all_predicted_vals)\n",
    "    share = 100*(variance_base_case/l2_loss.item())\n",
    "    \n",
    "    print(\" \")\n",
    "    print(f\"Road Type: {road_type}\")\n",
    "    \n",
    "    ###############################\n",
    "    ### For tables in the paper ###\n",
    "    ###############################\n",
    "\n",
    "    # # Table 6\n",
    "    # print(f\"{int(len(all_actual_vals)/416)} & {round(mean_car_vol,2)} & {variance_base_case:.2f} & {round(l2_loss.item(), 2)} & {round(l1_loss.item(), 2)} & {round(r_squared.item(), 2)} & {variance:.2f} \\\\\\\\\")\n",
    "    \n",
    "    # # Table 7\n",
    "    # print(f\"{len(all_actual_vals)} & {variance_base_case:.2f} & {variance:.2f} & {round(l2_loss.item(), 2)} & {round(l1_loss.item(), 2)} & {round(r_squared.item(), 2)} & {round(pearson_corr, 2)} & {round(spearman_corr, 2)} \\\\\\\\\")\n",
    "    \n",
    "    # # Table 8\n",
    "    # print(f\"{variance_base_case:.2f} [{ci_results['ci_sigma2'][0]:.2f}, {ci_results['ci_sigma2'][1]:.2f}] & {round(l2_loss.item(), 2)} [{ci_results['ci_mse'][0]:.2f}, {ci_results['ci_mse'][1]:.2f}] & {round(share, 2)}\\\\% [{ci_results['ci_share'][0]:.2f}, {ci_results['ci_share'][1]:.2f}] \\\\\\\\\")\n",
    "    \n",
    "    # # Table A.9\n",
    "    # print(f\"{int(len(all_actual_vals)/416)} & {round(mean_car_vol,2)} [{np.percentile(all_car_vols, 2.5):.2f}, {np.percentile(all_car_vols, 97.5):.2f}] & {variance_base_case:.2f} [{np.percentile(all_base_case_var, 2.5):.2f}, {np.percentile(all_base_case_var, 97.5):.2f}] \\\\\\\\\")\n",
    "\n",
    "    # print(f\"Number of observations: {len(all_actual_vals)/416}\")\n",
    "    print(f\"Number of observations: {len(all_actual_vals)}\")\n",
    "    print(f\"Mean Car Volume: {mean_car_vol}\")\n",
    "    # print(f\"Car Volume CI: [{np.percentile(all_car_vols, 2.5):.2f}, {np.percentile(all_car_vols, 97.5):.2f}]\")\n",
    "    # print(f\"Variance over car volumes: {car_volume_variance}\")\n",
    "    print(f\"R-squared: {round(r_squared.item(), 2)}\")\n",
    "    print(f\"MSE Loss: {round(l2_loss.item(), 2)}\")\n",
    "    print(f\"Naive MSE Loss: {l2_naive}\")\n",
    "    print(f\"Variance Base Case: {variance_base_case}\")\n",
    "    print(f\"Variance: {variance}\")\n",
    "    print(f\"L1 Loss: {round(l1_loss.item(), 2)}\")\n",
    "    print(f\"Naive L1 loss: {l1_naive}\")\n",
    "    print(f\"Standard Deviation Multiplied: {std_dev_multiplied}\")\n",
    "    print(f\"Spearman Correlation: {spearman_corr}\")\n",
    "    print(f\"Pearson Correlation: {pearson_corr}\")\n",
    "    print(f\"Standard Deviation: {std_dev}\")\n",
    "    print(f\"Coefficient of Variation: {cv_percent}\")\n",
    "    print(\" \")\n",
    "    \n",
    "    return {\n",
    "        'road_type': road_type,\n",
    "        'number_of_observations': len(all_actual_vals),\n",
    "        'mean_car_vol': mean_car_vol,\n",
    "        'r_squared': r_squared,\n",
    "        'mse': l2_loss,\n",
    "        'naive_mse': l2_naive,\n",
    "        'l1': l1_loss,\n",
    "        'naive_l1': l1_naive,\n",
    "        'variance': variance_base_case,\n",
    "        'std_dev': std_dev,\n",
    "        'std_dev_normalized': std_dev_multiplied,\n",
    "        'spearman': spearman_corr,\n",
    "        'pearson': pearson_corr,\n",
    "        'cv_percent': cv_percent\n",
    "    }\n",
    "\n",
    "# road_types = list(hf.get_road_type_indices(gdfs[0]).keys())\n",
    "road_types = [\"All Roads\",\"Trunk Roads\",\"Primary Roads\",\"Secondary Roads\",\"Tertiary Roads\",\"Residential Streets\",\"Living Streets\",\n",
    "              \"P/S/T Roads with Capacity Reduction\",\"P/S/T Roads with No Capacity Reduction\",\n",
    "              \"Primary Roads with Capacity Reduction\",\"Primary Roads with No Capacity Reduction\",\n",
    "              \"Secondary Roads with Capacity Reduction\",\"Secondary Roads with No Capacity Reduction\",\n",
    "              \"Tertiary Roads with Capacity Reduction\",\"Tertiary Roads with No Capacity Reduction\"]\n",
    "\n",
    "# Then calculate metrics for each road type\n",
    "metrics_by_type = {}\n",
    "for road_type in road_types:\n",
    "    metrics_by_type[road_type] = validate_model_with_interpretable_error(road_type, gdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More Plot Parameters for Radar and Scatter Plots\n",
    "# Tune as needed!\n",
    "\n",
    "selected_metrics = [       \n",
    "            {\n",
    "                'id': 'spearman',\n",
    "                'label': 'Spearman\\nCorrelation',\n",
    "                'transform': lambda x: max(0, x)\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                'id': 'r_squared',\n",
    "                'label': 'R²',\n",
    "                'transform': lambda x: max(0, x)\n",
    "            },\n",
    "            \n",
    "            {\n",
    "                'id': 'pearson',\n",
    "                'label': 'Pearson\\nCorrelation',\n",
    "                'transform': lambda x: max(0, x)\n",
    "            },\n",
    "        ]\n",
    "\n",
    "# Define selected road types\n",
    "selected_types = [\n",
    "    'Trunk Roads',\n",
    "    'Primary Roads',\n",
    "    'Secondary Roads',\n",
    "    'Tertiary Roads',\n",
    "    'Residential Streets',\n",
    "    'Living Streets',\n",
    "    'P/S/T Roads with Capacity Reduction',\n",
    "    'P/S/T Roads with No Capacity Reduction']\n",
    "\n",
    "colors = {\n",
    "    'Trunk Roads': '#d73027',         # Red\n",
    "    'Primary Roads': '#fc8d59',       # Orange\n",
    "    'Secondary Roads': '#fee090',     # Yellow\n",
    "    'Tertiary Roads': \"#a33aff\",      # Violet\n",
    "    'Residential Streets': '#91bfdb',  # Medium blue\n",
    "    'Living Streets': '#4575b4',      # Dark blue\n",
    "    'P/S/T Roads with Capacity Reduction': '#999999',    \n",
    "    'P/S/T Roads with No Capacity Reduction': \"#525252\",\n",
    "}\n",
    "\n",
    "# # Professional color palette with good contrast and accessibility\n",
    "# colors = {\n",
    "#     'Trunk Roads': '#1f77b4',         # Muted blue\n",
    "#     'Primary Roads': '#2ca02c',       # Muted green\n",
    "#     'Secondary Roads': '#ff7f0e',     # Muted orange\n",
    "#     'Tertiary Roads': '#9467bd',      # Muted purple\n",
    "#     'Residential Streets': '#8c564b',  # Brown\n",
    "#     'Living Streets': '#e377c2',      # Pink\n",
    "#     'P/S/T Roads with Capacity Reduction': '#7f7f7f',    # Gray\n",
    "#     'P/S/T Roads with No Capacity Reduction': '#bcbd22', # Olive\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar Plot\n",
    "pf.create_correlation_radar_plot_sort_by_r2(metrics_by_type, selected_metrics, result_path=result_path, save_it=True, selected_types=selected_types, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error vs Variability Scatterplot\n",
    "manual_label_offsets = {\n",
    "    'Trunk Roads': (-80, 7),\n",
    "    'Primary Roads': (9, 10),\n",
    "    'Secondary Roads': (-115, 10),\n",
    "    'Tertiary Roads': (10, -20),\n",
    "    'Residential Streets': (-115, 19),\n",
    "    'Living Streets': (10, -25),\n",
    "    'P/S/T Roads with Capacity Reduction': (-235, -15),\n",
    "    'P/S/T Roads with No Capacity Reduction': (8, 15)}\n",
    "\n",
    "pf.create_error_vs_variability_scatterplots(metrics_by_type, manual_label_offsets, result_path=result_path, save_it=True, selected_types=selected_types, colors=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the distribution of ERRORS\n",
    "def analyze_error_distribution(road_type, gdfs):\n",
    "\n",
    "    all_actuals = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    for gdf in gdfs:\n",
    "        \n",
    "        indices = hf.get_road_type_indices(gdf)[road_type]\n",
    "    \n",
    "        actual = gdf.loc[indices, 'vol_car_change_actual']\n",
    "        predicted = gdf.loc[indices, 'vol_car_change_predicted']\n",
    "        \n",
    "        all_actuals.extend(actual.to_numpy())\n",
    "        all_predictions.extend(predicted.to_numpy())\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = np.array(all_predictions) - np.array(all_actuals)\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.rcParams['font.family'] = 'Times New Roman'\n",
    "    plt.rcParams['font.size'] = 14\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(errors, bins=64, edgecolor='black')\n",
    "    \n",
    "    # Add mean ± std, min, max as text\n",
    "    mean_err = errors.mean()\n",
    "    std_err = errors.std()\n",
    "    min_err = errors.min()\n",
    "    max_err = errors.max()\n",
    "    median_err = np.median(errors)\n",
    "    \n",
    "    # Add stats as text in the plot\n",
    "    # Remove negative sign if mean or median is -0.00 after rounding\n",
    "    mean_str = f\"{mean_err:.2f}\"\n",
    "    if mean_str == \"-0.00\":\n",
    "        mean_str = \"0\"\n",
    "    \n",
    "    median_str = f\"{median_err:.2f}\"\n",
    "    if median_str == \"-0.00\":\n",
    "        median_str = \"0\"\n",
    "    \n",
    "    stats_text = (\n",
    "        f\"Mean: {mean_str}\\n\"\n",
    "        f\"σ: {std_err:.2f}\\n\"\n",
    "        f\"Within ±1 σ: {(abs(errors) <= std_err).mean()*100:.1f}%\\n\\n\"\n",
    "        f\"Median: {median_str}\\n\"\n",
    "        f\"Min: {min_err:.2f}\\n\"\n",
    "        f\"Max: {max_err:.2f}\"\n",
    "    )\n",
    "    plt.gca().text(\n",
    "        0.98, 0.98, stats_text,\n",
    "        transform=plt.gca().transAxes,\n",
    "        fontsize=13,\n",
    "        verticalalignment='top',\n",
    "        horizontalalignment='right',\n",
    "        bbox=dict(facecolor='white', alpha=0.7, edgecolor='gray')\n",
    "    )\n",
    "    \n",
    "    plt.xlabel(\"Prediction Error\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.savefig(f\"{result_path}/errors_hist_{road_type}.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "analyze_error_distribution(\"Living Streets\", gdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_receptive_field_hist(edge_index, num_nodes, max_hops=10):\n",
    "    \n",
    "    hist = defaultdict(list)\n",
    "\n",
    "    for node_id in tqdm(range(num_nodes)):\n",
    "        for k in list(range(3, max_hops + 1, 2)) + [54]:\n",
    "            subset, _, _, _ = k_hop_subgraph(\n",
    "                node_id, k, edge_index, relabel_nodes=False\n",
    "            )\n",
    "            hist[k].append(len(subset))  # Store receptive field size\n",
    "\n",
    "    return hist\n",
    "\n",
    "def plot_histogram(hist):\n",
    "    avg_sizes = [sum(hist[k]) / len(hist[k]) for k in sorted(hist)]\n",
    "    std_sizes = [torch.tensor(hist[k]).float().std().item() for k in sorted(hist)]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    plt.errorbar(\n",
    "        sorted(hist),\n",
    "        avg_sizes,\n",
    "        yerr=std_sizes,\n",
    "        fmt='-o',\n",
    "        capsize=5,\n",
    "        ecolor='gray',\n",
    "        label='Avg Receptive Field Size ± std'\n",
    "    )\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    plt.xlabel(\"Number of Hops\", fontsize=15)\n",
    "    plt.ylabel(\"Avg Receptive Field Size\", fontsize=15)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"receptive_fields.png\", bbox_inches='tight', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming it's consistent!\n",
    "edge_index = test_set_dl[0].edge_index\n",
    "num_nodes = 31559\n",
    "\n",
    "hist = compute_receptive_field_hist(edge_index, num_nodes, max_hops=51)\n",
    "plot_histogram(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Analysis (Some Code Repetition?!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_volumes_by_capacity_reduction(gdf):\n",
    "    \"\"\"\n",
    "    Compare volumes between roads with and without capacity reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Should contain:\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with comparison statistics\n",
    "    \"\"\"\n",
    "    # Create masks for roads with and without capacity reduction\n",
    "    has_reduction = gdf['capacity_reduction'] < 0\n",
    "    no_reduction = gdf['capacity_reduction'] >= 0\n",
    "    \n",
    "    # Calculate statistics for each group\n",
    "    reduced_stats = {\n",
    "        'mean_volume': gdf[has_reduction]['vol_base_case'].mean(),\n",
    "        'std_volume': gdf[has_reduction]['vol_base_case'].std(),\n",
    "        'count': has_reduction.sum()\n",
    "    }\n",
    "    \n",
    "    normal_stats = {\n",
    "        'mean_volume': gdf[no_reduction]['vol_base_case'].mean(),\n",
    "        'std_volume': gdf[no_reduction]['vol_base_case'].std(),\n",
    "        'count': no_reduction.sum()\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'with_reduction': reduced_stats,\n",
    "        'without_reduction': normal_stats\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "comparison = analyze_volumes_by_capacity_reduction(gdfs[0])\n",
    "print(\"\\nComparison of roads with and without capacity reduction:\")\n",
    "print(\"\\nRoads WITH capacity reduction:\")\n",
    "print(f\"  Mean volume: {comparison['with_reduction']['mean_volume']:.2f}\")\n",
    "print(f\"  Std volume: {comparison['with_reduction']['std_volume']:.2f}\")\n",
    "print(f\"  Count: {comparison['with_reduction']['count']}\")\n",
    "print(\"\\nRoads WITHOUT capacity reduction:\")\n",
    "print(f\"  Mean volume: {comparison['without_reduction']['mean_volume']:.2f}\")\n",
    "print(f\"  Std volume: {comparison['without_reduction']['std_volume']:.2f}\")\n",
    "print(f\"  Count: {comparison['without_reduction']['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_variances_by_road_type_and_reduction(gdfs):\n",
    "    # Initialize dictionaries for each road type\n",
    "    road_types = {\n",
    "        1: \"Primary\",\n",
    "        2: \"Secondary\",\n",
    "        3: \"Tertiary\"\n",
    "    }\n",
    "    \n",
    "    stats = {road_type: {\n",
    "        'with_reduction': {'actual_changes': [], 'predictions': [], 'errors': []},\n",
    "        'without_reduction': {'actual_changes': [], 'predictions': [], 'errors': []}\n",
    "    } for road_type in road_types.keys()}\n",
    "    \n",
    "    # Collect values from all GDFs\n",
    "    for gdf in gdfs:\n",
    "        \n",
    "        for road_type in road_types.keys():\n",
    "            # Filter for specific road type\n",
    "            road_type_mask = gdf['highway'] == road_type\n",
    "            \n",
    "            # Roads with capacity reduction\n",
    "            reduced = (gdf['capacity_reduction_rounded'] < -1e-3) & road_type_mask\n",
    "            stats[road_type]['with_reduction']['actual_changes'].extend(gdf.loc[reduced, 'vol_car_change_actual'])\n",
    "            stats[road_type]['with_reduction']['predictions'].extend(gdf.loc[reduced, 'vol_car_change_predicted'])\n",
    "            stats[road_type]['with_reduction']['errors'].extend(\n",
    "                gdf.loc[reduced, 'vol_car_change_predicted'] - gdf.loc[reduced, 'vol_car_change_actual']\n",
    "            )\n",
    "            \n",
    "            # Roads without capacity reduction\n",
    "            not_reduced = (gdf['capacity_reduction_rounded'] >= -1e-3) & road_type_mask\n",
    "            stats[road_type]['without_reduction']['actual_changes'].extend(gdf.loc[not_reduced, 'vol_car_change_actual'])\n",
    "            stats[road_type]['without_reduction']['predictions'].extend(gdf.loc[not_reduced, 'vol_car_change_predicted'])\n",
    "            stats[road_type]['without_reduction']['errors'].extend(\n",
    "                gdf.loc[not_reduced, 'vol_car_change_predicted'] - gdf.loc[not_reduced, 'vol_car_change_actual']\n",
    "            )\n",
    "    \n",
    "    # Calculate and print statistics for each road type\n",
    "    for road_type in road_types.keys():\n",
    "        print(f\"\\n{road_types[road_type]} Roads:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for reduction_type in ['with_reduction', 'without_reduction']:\n",
    "            # Convert to numpy arrays\n",
    "            actual = np.array(stats[road_type][reduction_type]['actual_changes'])\n",
    "            errors = np.array(stats[road_type][reduction_type]['errors'])\n",
    "            \n",
    "            if len(actual) > 0:  # Only print if we have data\n",
    "                print(f\"\\n{reduction_type.replace('_', ' ').title()}:\")\n",
    "                print(f\"Number of observations: {len(actual)}\")\n",
    "                print(f\"Variance of actual changes: {np.var(actual):.2f}\")\n",
    "                print(f\"Mean absolute error: {np.mean(np.abs(errors)):.2f}\")\n",
    "                print(f\"MSE: {np.mean(errors**2):.2f}\")\n",
    "                print(f\"Mean actual change: {np.mean(actual):.2f}\")\n",
    "                print(f\"Std of actual changes: {np.std(actual):.2f}\")\n",
    "                print(f\"R² score: {1 - np.mean(errors**2)/np.var(actual):.3f}\")\n",
    "\n",
    "analyze_variances_by_road_type_and_reduction(gdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_analysis(gdf):\n",
    "    \"\"\"\n",
    "    Perform comprehensive statistical analysis comparing roads with and without capacity reduction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        Should contain:\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with detailed statistical analysis\n",
    "    \"\"\"\n",
    "    import scipy.stats as stats\n",
    "    import numpy as np\n",
    "    \n",
    "    # Create masks for roads with and without capacity reduction\n",
    "    has_reduction = gdf['capacity_reduction'] < 0\n",
    "    no_reduction = gdf['capacity_reduction'] >= 0\n",
    "    \n",
    "    # Get volumes for each group\n",
    "    volumes_reduced = gdf[has_reduction]['vol_base_case'].dropna()\n",
    "    volumes_normal = gdf[no_reduction]['vol_base_case'].dropna()\n",
    "    \n",
    "    # Basic statistics\n",
    "    stats_reduced = {\n",
    "        'count': len(volumes_reduced),\n",
    "        'mean': volumes_reduced.mean(),\n",
    "        'median': volumes_reduced.median(),\n",
    "        'std': volumes_reduced.std(),\n",
    "        'q25': volumes_reduced.quantile(0.25),\n",
    "        'q75': volumes_reduced.quantile(0.75),\n",
    "        'skew': stats.skew(volumes_reduced),\n",
    "        'kurtosis': stats.kurtosis(volumes_reduced)\n",
    "    }\n",
    "    \n",
    "    stats_normal = {\n",
    "        'count': len(volumes_normal),\n",
    "        'mean': volumes_normal.mean(),\n",
    "        'median': volumes_normal.median(),\n",
    "        'std': volumes_normal.std(),\n",
    "        'q25': volumes_normal.quantile(0.25),\n",
    "        'q75': volumes_normal.quantile(0.75),\n",
    "        'skew': stats.skew(volumes_normal),\n",
    "        'kurtosis': stats.kurtosis(volumes_normal)\n",
    "    }\n",
    "    \n",
    "    # Perform Mann-Whitney U test (non-parametric test for different distributions)\n",
    "    mw_stat, mw_pval = stats.mannwhitneyu(volumes_reduced, volumes_normal, alternative='two-sided')\n",
    "    \n",
    "    # Calculate effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((stats_reduced['count'] - 1) * stats_reduced['std']**2 + \n",
    "                         (stats_normal['count'] - 1) * stats_normal['std']**2) / \n",
    "                        (stats_reduced['count'] + stats_normal['count'] - 2))\n",
    "    cohens_d = (stats_reduced['mean'] - stats_normal['mean']) / pooled_std\n",
    "    \n",
    "    return {\n",
    "        'with_reduction': stats_reduced,\n",
    "        'without_reduction': stats_normal,\n",
    "        'mann_whitney': {\n",
    "            'statistic': mw_stat,\n",
    "            'p_value': mw_pval\n",
    "        },\n",
    "        'cohens_d': cohens_d\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "analysis = perform_statistical_analysis(gdfs[0])\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDetailed Statistical Analysis:\")\n",
    "print(\"\\nRoads WITH capacity reduction:\")\n",
    "print(f\"  Count: {analysis['with_reduction']['count']}\")\n",
    "print(f\"  Mean: {analysis['with_reduction']['mean']:.2f}\")\n",
    "print(f\"  Median: {analysis['with_reduction']['median']:.2f}\")\n",
    "print(f\"  Std Dev: {analysis['with_reduction']['std']:.2f}\")\n",
    "print(f\"  Q25-Q75: [{analysis['with_reduction']['q25']:.2f} - {analysis['with_reduction']['q75']:.2f}]\")\n",
    "print(f\"  Skewness: {analysis['with_reduction']['skew']:.2f}\")\n",
    "\n",
    "print(\"\\nRoads WITHOUT capacity reduction:\")\n",
    "print(f\"  Count: {analysis['without_reduction']['count']}\")\n",
    "print(f\"  Mean: {analysis['without_reduction']['mean']:.2f}\")\n",
    "print(f\"  Median: {analysis['without_reduction']['median']:.2f}\")\n",
    "print(f\"  Std Dev: {analysis['without_reduction']['std']:.2f}\")\n",
    "print(f\"  Q25-Q75: [{analysis['without_reduction']['q25']:.2f} - {analysis['without_reduction']['q75']:.2f}]\")\n",
    "print(f\"  Skewness: {analysis['without_reduction']['skew']:.2f}\")\n",
    "\n",
    "print(\"\\nStatistical Tests:\")\n",
    "print(f\"  Mann-Whitney U p-value: {analysis['mann_whitney']['p_value']:.10f}\")\n",
    "print(f\"  Effect size (Cohen's d): {analysis['cohens_d']:.2f}\")\n",
    "\n",
    "# Print interpretation\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"1. Statistical Significance:\")\n",
    "if analysis['mann_whitney']['p_value'] < 0.05:\n",
    "    print(\"  - The difference in volumes is statistically significant (p < 0.05)\")\n",
    "else:\n",
    "    print(\"  - The difference in volumes is not statistically significant (p >= 0.05)\")\n",
    "\n",
    "print(\"\\n2. Effect Size Interpretation:\")\n",
    "d = abs(analysis['cohens_d'])\n",
    "if d < 0.2:\n",
    "    effect = \"negligible\"\n",
    "elif d < 0.5:\n",
    "    effect = \"small\"\n",
    "elif d < 0.8:\n",
    "    effect = \"medium\"\n",
    "else:\n",
    "    effect = \"large\"\n",
    "print(f\"  - Cohen's d = {d:.2f} indicates a {effect} effect size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Analysis\n",
    "Do all districts appear with the same frequency in the test set? Investigated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze district-level patterns\n",
    "def analyze_district_selection_bias(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze if districts with higher volumes/variances are selected more often.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics and selection patterns\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate base case statistics\n",
    "        mean_volume = district_data['vol_base_case'].mean()\n",
    "        total_volume = district_data['vol_base_case'].sum()\n",
    "        volume_variance = district_data['vol_base_case'].var()\n",
    "        n_roads = len(district_data)\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'total_volume': total_volume,\n",
    "            'volume_variance': volume_variance,\n",
    "            'n_roads': n_roads,\n",
    "            'selection_frequency': selections,\n",
    "            'selection_probability': selections / len(gdf_inputs)\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Calculate correlations\n",
    "    volume_selection_corr = df_stats['total_volume'].corr(df_stats['selection_frequency'])\n",
    "    variance_selection_corr = df_stats['volume_variance'].corr(df_stats['selection_frequency'])\n",
    "    \n",
    "    # Sort districts by selection frequency\n",
    "    df_stats_sorted = df_stats.sort_values('selection_frequency', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'district_stats': df_stats_sorted,\n",
    "        'correlations': {\n",
    "            'volume_selection': volume_selection_corr,\n",
    "            'variance_selection': variance_selection_corr\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "analysis = analyze_district_selection_bias(gdfs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDistrict Statistics (sorted by selection frequency):\")\n",
    "print(analysis['district_stats'])\n",
    "\n",
    "print(\"\\nCorrelations:\")\n",
    "print(f\"Volume vs Selection Frequency: {analysis['correlations']['volume_selection']:.3f}\")\n",
    "print(f\"Variance vs Selection Frequency: {analysis['correlations']['variance_selection']:.3f}\")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Volume vs Selection Frequency\n",
    "ax1.scatter(analysis['district_stats']['total_volume'], \n",
    "           analysis['district_stats']['selection_frequency'])\n",
    "ax1.set_xlabel('Total Base Case Volume')\n",
    "ax1.set_ylabel('Selection Frequency')\n",
    "ax1.set_title('Volume vs Selection Frequency')\n",
    "\n",
    "# Variance vs Selection Frequency\n",
    "ax2.scatter(analysis['district_stats']['volume_variance'], \n",
    "           analysis['district_stats']['selection_frequency'])\n",
    "ax2.set_xlabel('Base Case Volume Variance')\n",
    "ax2.set_ylabel('Selection Frequency')\n",
    "ax2.set_title('Variance vs Selection Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze district-level patterns focusing on volumes\n",
    "def analyze_district_volume_bias(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze if districts with higher volumes are selected more often.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics and selection patterns\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate base case volume statistics\n",
    "        mean_volume = district_data['vol_base_case'].mean()\n",
    "        total_volume = district_data['vol_base_case'].sum()\n",
    "        n_roads = len(district_data)\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'total_volume': total_volume,\n",
    "            'n_roads': n_roads,\n",
    "            'selection_frequency': selections,\n",
    "            'selection_probability': selections / len(gdf_inputs)\n",
    "        }\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Calculate correlation\n",
    "    volume_selection_corr = df_stats['total_volume'].corr(df_stats['selection_frequency'])\n",
    "    \n",
    "    # Sort districts by volume\n",
    "    df_stats_sorted = df_stats.sort_values('total_volume', ascending=False)\n",
    "    \n",
    "    return {\n",
    "        'district_stats': df_stats_sorted,\n",
    "        'correlation': volume_selection_corr\n",
    "    }\n",
    "\n",
    "# Run the analysis\n",
    "volume_analysis = analyze_district_volume_bias(gdfs)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nDistrict Statistics (sorted by total volume):\")\n",
    "print(volume_analysis['district_stats'])\n",
    "\n",
    "print(\"\\nCorrelation between Volume and Selection Frequency:\", \n",
    "      f\"{volume_analysis['correlation']:.3f}\")\n",
    "\n",
    "# Plot relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(volume_analysis['district_stats']['total_volume'], \n",
    "           volume_analysis['district_stats']['selection_frequency'])\n",
    "plt.xlabel('Total Base Case Volume')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('District Volume vs Selection Frequency')\n",
    "\n",
    "# Add district labels to points\n",
    "for idx, row in volume_analysis['district_stats'].iterrows():\n",
    "    plt.annotate(f\"District {idx}\", \n",
    "                (row['total_volume'], row['selection_frequency']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze volume distributions within districts\n",
    "def analyze_within_district_volumes(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze how volumes are distributed within each district.\n",
    "    \"\"\"\n",
    "    # Get first GDF as reference for base case\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Calculate district-level statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate volume statistics\n",
    "        volume_stats = {\n",
    "            'mean': district_data['vol_base_case'].mean(),\n",
    "            'median': district_data['vol_base_case'].median(),\n",
    "            'std': district_data['vol_base_case'].std(),\n",
    "            'q25': district_data['vol_base_case'].quantile(0.25),\n",
    "            'q75': district_data['vol_base_case'].quantile(0.75),\n",
    "            'skew': district_data['vol_base_case'].skew(),\n",
    "            'n_roads': len(district_data),\n",
    "            'n_high_volume': len(district_data[district_data['vol_base_case'] > district_data['vol_base_case'].mean()]),\n",
    "            'total_volume': district_data['vol_base_case'].sum()\n",
    "        }\n",
    "        \n",
    "        # Count selections across scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            has_reduction = (district_data['capacity_reduction'] < 0).any()\n",
    "            if has_reduction:\n",
    "                selections += 1\n",
    "        \n",
    "        volume_stats['selection_frequency'] = selections\n",
    "        district_stats[district] = volume_stats\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df_stats = pd.DataFrame.from_dict(district_stats, orient='index')\n",
    "    \n",
    "    # Sort by total volume\n",
    "    df_stats_sorted = df_stats.sort_values('total_volume', ascending=False)\n",
    "    \n",
    "    # Calculate proportion of high-volume roads\n",
    "    df_stats_sorted['prop_high_volume'] = df_stats_sorted['n_high_volume'] / df_stats_sorted['n_roads']\n",
    "    \n",
    "    return df_stats_sorted\n",
    "\n",
    "# Run the analysis\n",
    "district_volume_analysis = analyze_within_district_volumes(gdfs)\n",
    "\n",
    "print(\"\\nDistrict Volume Distribution Analysis:\")\n",
    "print(district_volume_analysis)\n",
    "\n",
    "# Create visualization of volume distributions\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Distribution skewness vs selection frequency\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(district_volume_analysis['skew'], \n",
    "           district_volume_analysis['selection_frequency'])\n",
    "plt.xlabel('Volume Distribution Skewness')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('Skewness vs Selection Frequency')\n",
    "\n",
    "# Plot 2: Proportion of high-volume roads vs selection frequency\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.scatter(district_volume_analysis['prop_high_volume'], \n",
    "           district_volume_analysis['selection_frequency'])\n",
    "plt.xlabel('Proportion of High-Volume Roads')\n",
    "plt.ylabel('Selection Frequency')\n",
    "plt.title('High-Volume Road Proportion vs Selection Frequency')\n",
    "\n",
    "# Plot 3: Box plot of volume distributions by selection frequency quartile\n",
    "plt.subplot(2, 2, 3)\n",
    "district_volume_analysis['selection_quartile'] = pd.qcut(district_volume_analysis['selection_frequency'], \n",
    "                                                       q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "selection_groups = district_volume_analysis.groupby('selection_quartile')\n",
    "box_data = [group['mean'] for name, group in selection_groups]\n",
    "plt.boxplot(box_data, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "plt.xlabel('Selection Frequency Quartile')\n",
    "plt.ylabel('Mean Volume')\n",
    "plt.title('Volume Distribution by Selection Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nSummary of findings:\")\n",
    "print(\"\\n1. Volume distribution characteristics:\")\n",
    "print(f\"Mean skewness across districts: {district_volume_analysis['skew'].mean():.2f}\")\n",
    "print(f\"Mean proportion of high-volume roads: {district_volume_analysis['prop_high_volume'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n2. Correlations with selection frequency:\")\n",
    "print(f\"Skewness correlation: {district_volume_analysis['skew'].corr(district_volume_analysis['selection_frequency']):.3f}\")\n",
    "print(f\"High-volume proportion correlation: {district_volume_analysis['prop_high_volume'].corr(district_volume_analysis['selection_frequency']):.3f}\")\n",
    "\n",
    "# Group districts by selection frequency quartile and analyze volume patterns\n",
    "quartile_stats = district_volume_analysis.groupby('selection_quartile').agg({\n",
    "    'mean': 'mean',\n",
    "    'std': 'mean',\n",
    "    'skew': 'mean',\n",
    "    'prop_high_volume': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\n3. Statistics by selection frequency quartile:\")\n",
    "print(quartile_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_district_volumes(gdf_inputs):\n",
    "    \"\"\"\n",
    "    Analyze volume distribution across districts with robust nan handling.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf_inputs : list of GeoDataFrames\n",
    "        Each GDF should contain:\n",
    "        - district information\n",
    "        - vol_base_case (base volume)\n",
    "        - capacity_reduction (to identify selected districts)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with district statistics\n",
    "    \"\"\"\n",
    "    if not gdf_inputs or len(gdf_inputs) == 0:\n",
    "        raise ValueError(\"Empty input list provided\")\n",
    "\n",
    "    # Get first GDF as reference\n",
    "    base_gdf = gdf_inputs[0].copy()\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_columns = ['district', 'vol_base_case', 'capacity_reduction']\n",
    "    if not all(col in base_gdf.columns for col in required_columns):\n",
    "        raise ValueError(f\"Missing required columns. Expected: {required_columns}\")\n",
    "    \n",
    "    # Calculate district statistics\n",
    "    district_stats = {}\n",
    "    \n",
    "    # For each district\n",
    "    for district in base_gdf['district'].unique():\n",
    "        if pd.isna(district):  # Skip if district is nan\n",
    "            continue\n",
    "            \n",
    "        district_mask = base_gdf['district'] == district\n",
    "        district_data = base_gdf[district_mask]\n",
    "        \n",
    "        # Calculate mean volume with nan handling\n",
    "        volumes = district_data['vol_base_case'].dropna()\n",
    "        if len(volumes) == 0:\n",
    "            mean_volume = np.nan\n",
    "        else:\n",
    "            mean_volume = volumes.mean()\n",
    "        \n",
    "        # Count edges (excluding nan values)\n",
    "        n_edges = len(district_data.dropna(subset=['vol_base_case']))\n",
    "        \n",
    "        # Count selections across all scenarios\n",
    "        selections = 0\n",
    "        for gdf in gdf_inputs:\n",
    "            if 'district' not in gdf.columns or 'capacity_reduction' not in gdf.columns:\n",
    "                continue\n",
    "                \n",
    "            district_data = gdf[gdf['district'] == district]\n",
    "            # Count how often this district has capacity reductions, handling nans\n",
    "            capacity_reductions = district_data['capacity_reduction'].dropna()\n",
    "            if len(capacity_reductions) > 0 and (capacity_reductions < 0).any():\n",
    "                selections += 1\n",
    "        \n",
    "        district_stats[district] = {\n",
    "            'mean_volume': mean_volume,\n",
    "            'n_edges': n_edges,\n",
    "            'selection_frequency': selections\n",
    "        }\n",
    "    \n",
    "    # Calculate overall statistics, excluding nan values\n",
    "    valid_volumes = [stats['mean_volume'] for stats in district_stats.values() \n",
    "                    if not np.isnan(stats['mean_volume'])]\n",
    "    \n",
    "    if not valid_volumes:\n",
    "        return {\n",
    "            'district_stats': district_stats,\n",
    "            'mean_volume': np.nan,\n",
    "            'std_volume': np.nan,\n",
    "            'threshold': np.nan,\n",
    "            'high_volume_districts': {}\n",
    "        }\n",
    "    \n",
    "    mean_volume = np.mean(valid_volumes)\n",
    "    std_volume = np.std(valid_volumes)\n",
    "    threshold = mean_volume + std_volume\n",
    "    \n",
    "    # Identify high-volume districts, excluding nan values\n",
    "    high_volume_districts = {\n",
    "        d: stats for d, stats in district_stats.items() \n",
    "        if not np.isnan(stats['mean_volume']) and stats['mean_volume'] > threshold\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'district_stats': district_stats,\n",
    "        'mean_volume': mean_volume,\n",
    "        'std_volume': std_volume,\n",
    "        'threshold': threshold,\n",
    "        'high_volume_districts': high_volume_districts\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "stats = analyze_district_volumes(gdfs)\n",
    "print(f\"Mean volume across districts: {stats['mean_volume']:.2f}\")\n",
    "print(f\"Standard deviation: {stats['std_volume']:.2f}\")\n",
    "print(f\"Threshold for high-volume: {stats['threshold']:.2f}\")\n",
    "print(\"\\nHigh-volume districts:\")\n",
    "for district, data in stats['high_volume_districts'].items():\n",
    "    print(f\"District {district}:\")\n",
    "    print(f\"  Mean volume: {data['mean_volume']:.2f}\")\n",
    "    print(f\"  Number of edges: {data['n_edges']}\")\n",
    "    print(f\"  Selection frequency: {data['selection_frequency']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between volume and variance\n",
    "volume_variance_corr = analysis['district_stats']['total_volume'].corr(analysis['district_stats']['volume_variance'])\n",
    "print(\"\\nCorrelation between Volume and Variance:\", f\"{volume_variance_corr:.3f}\")\n",
    "\n",
    "# Create scatter plot of volume vs variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(analysis['district_stats']['total_volume'], \n",
    "           analysis['district_stats']['volume_variance'])\n",
    "plt.xlabel('Total Base Case Volume')\n",
    "plt.ylabel('Volume Variance')\n",
    "plt.title('District Volume vs Variance')\n",
    "\n",
    "# Add district labels to points\n",
    "for idx, row in analysis['district_stats'].iterrows():\n",
    "    plt.annotate(f\"District {idx}\", \n",
    "                (row['total_volume'], row['volume_variance']))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chenhao-gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
